{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895050c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    ")\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd82514f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelBERT_Political(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name,\n",
    "        num_stance_labels,\n",
    "        num_group_appeals_labels,\n",
    "        num_endorsement_labels,\n",
    "    ):\n",
    "        super(MultiLabelBERT_Political, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.stance_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_stance_labels\n",
    "        )\n",
    "        self.group_appeals_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_group_appeals_labels\n",
    "        )\n",
    "        self.endorsement_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_endorsement_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        stance_logits = self.stance_classifier(sequence_output)\n",
    "        group_appeals_logits = self.group_appeals_classifier(sequence_output)\n",
    "        endorsement_logits = self.endorsement_classifier(sequence_output)\n",
    "\n",
    "        return stance_logits, group_appeals_logits, endorsement_logits\n",
    "\n",
    "class MultiLabelBERT_Sentiment(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_name, num_hyperbol_labels, num_sentiment_labels):\n",
    "        super(MultiLabelBERT_Sentiment, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.hyperbole_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_hyperbol_labels\n",
    "        )\n",
    "        self.sentiment_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_sentiment_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        hyperbole_logits = self.hyperbole_classifier(sequence_output)\n",
    "        sentiment_logits = self.sentiment_classifier(sequence_output)\n",
    "\n",
    "        return hyperbole_logits, sentiment_logits\n",
    "\n",
    "class MultiLabelBERT_Readability(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_name, num_vagueness_labels, num_readability_labels):\n",
    "        super(MultiLabelBERT_Readability, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.vagueness_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_vagueness_labels\n",
    "        )\n",
    "        self.readability_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_readability_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        vagueness_logits = self.vagueness_classifier(sequence_output)\n",
    "        readability_logits = self.readability_classifier(sequence_output)\n",
    "\n",
    "        return vagueness_logits, readability_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "977752c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_path = \"group_1_model.pth\"\n",
    "model_2_path = \"group_2_model.pth\"\n",
    "model_3_path = \"group_3_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "927f4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\AppData\\Local\\Temp\\ipykernel_10908\\1866976455.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_1.load_state_dict(torch.load(model_1_path))\n",
      "C:\\Users\\rishi\\AppData\\Local\\Temp\\ipykernel_10908\\1866976455.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_2.load_state_dict(torch.load(model_2_path))\n",
      "C:\\Users\\rishi\\AppData\\Local\\Temp\\ipykernel_10908\\1866976455.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_3.load_state_dict(torch.load(model_3_path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiLabelBERT_Readability(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (vagueness_classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (readability_classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load models from file\n",
    "model_1 = MultiLabelBERT_Political(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    num_stance_labels=3,\n",
    "    num_group_appeals_labels=2,\n",
    "    num_endorsement_labels=2,\n",
    ")\n",
    "model_1.load_state_dict(torch.load(model_1_path))\n",
    "model_1.eval()\n",
    "model_2 = MultiLabelBERT_Sentiment(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    num_hyperbol_labels=2,\n",
    "    num_sentiment_labels=6,\n",
    ")\n",
    "model_2.load_state_dict(torch.load(model_2_path))\n",
    "model_2.eval()\n",
    "model_3 = MultiLabelBERT_Readability(\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    num_vagueness_labels=2,\n",
    "    num_readability_labels=6,\n",
    ")\n",
    "model_3.load_state_dict(torch.load(model_3_path))\n",
    "model_3.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_1.to(device)\n",
    "model_2.to(device)\n",
    "model_3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebaae54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fd92a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file saved to output/output.json\n"
     ]
    }
   ],
   "source": [
    "text = \"An Australian father who is believed to have been brainwashed by the preachings of Islamic extremist Musa Cerantonio has joined the ranks of terror group Islamic State to fight in the Middle East. According to Twitter, Zia Abdul Haq's current location is in an IS-controlled area of Ash-Shaam, also known as Syria. Friends of the 33-year-old believed the man from Logan - south of Brisbane - was moving to Syria to look for a new wife after the breakdown of his marriage, but The Courier Mail has revealed Abdul Haq is in fact fighting with the extremist organisation. Scroll down for video . The friends of Zia Abdul Haq says the father-of-one, from Logan, is a 'good-hearted guy' Two of Australia's most notorious jihadists Khaled Sharrouf and Mohamed Elomar are part of the same group. Abdul Haq is one of the latest of three men who are believed to have joined IS, including Melbourne's Abu Khaled al-Cambodi and Mounir Raad. The 33-year-old's friends described him as a 'good-hearted guy' but had been 'brainwashed' by Cerantonio - who is believed to be an influential IS figure - after hearing the radical sheik speak in Brisbane two years ago. The 33-year-old's friends claim he was brainwashed by prominent IS supporter, Musa Cerantonio - a radical sheik . 'He wasn’t a bad guy but he fell off the rails and under the spell of the extremists,' friend Yehya El-Kholed told The Courier Mail. 'Sheik Cerantonio is a hypocrite because while he urges people to go to Syria, he won’t go himself.' Since he landed in Syria, Abdul Haq - who came to Australia in his 20s and took up a job as a finance officer - has been actively posting photos depicting life as a fighter for IS on the social media site. These include M16 training, jihadists playing violent video games and cooking up a shared meal. But like Sharrouf and Elomar, Abdul Haq - who has a young son - has used Twitter as platform to fire off threatening messages, including one to foreign forces to 'bring dead body bag for each of your soldiers'. Most recently, he took aim at the U.S. saying 'Send you (men) to fight us, your drones are useless on the ground'  and 'How come you fight #IS with your gays & lesbians army' on Saturday night. Abdul Haq has also hit back at media outlets circulating news about him. 'Many lies about me in Australian news. =/,' he tweeted within half an hour of reports emerging. Since arriving in Syria Abdul Haq has been posting photos on Twitter of what it is like being an IS fighter . Last month Musa Cerantonio was deported back to Australia from the Philippines after he was arrested after breaching the Philippines' immigration laws. Changing his name from Robert Edward, the 29-year-old was detained in the central Filipino city of Cebu where he was believed to have been hiding from Australian authorities, despite claiming on Twitter he had arrived in the Middle East just days before his arrest. He was found with a Filipino fashion designer claiming to be his wife, inside an apartment. Australian authorities cancelled his passport, triggering his arrest and leaving without any documentation. He was then set free when he arrived in Melbourne after deportation as he had not broken any Australian laws. Musa Cerantonio is thought to be an influential IS figure and Abdul Haq is believed to have taken up extremist views after hearing the sheik speak in 2012 in Brisbane .\"\n",
    "inputs = tokenizer(\n",
    "    text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    ")\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    stance_logits, group_appeals_logits, endorsement_logits = model_1(\n",
    "        inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "    )\n",
    "    hyperbole_logits, sentiment_logits = model_2(\n",
    "        inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "    )\n",
    "    vagueness_logits, readability_logits = model_3(\n",
    "        inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    pred_labels_stance = torch.argmax(stance_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_group_appeals = torch.argmax(group_appeals_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_endorsement = torch.argmax(endorsement_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_hyperbole = torch.argmax(hyperbole_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_sentiment = torch.argmax(sentiment_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_vagueness = torch.argmax(vagueness_logits, dim=2).squeeze().cpu().numpy()\n",
    "    pred_labels_readability = torch.argmax(readability_logits, dim=2).squeeze().cpu().numpy()\n",
    "\n",
    "    # get labels\n",
    "    stance_labels = [\"Left\", \"Center\", \"Right\"]\n",
    "    group_appeals_labels = [\"No Group Appeal\", \"Group Appeal\"]\n",
    "    endorsement_labels = [\"No Endorsement\", \"Endorsement\"]\n",
    "    hyperbole_labels = [\"No Hyperbole\", \"Hyperbole\"]\n",
    "    sentiment_labels = [\n",
    "        \"Joy\",\n",
    "        \"Sadness\",\n",
    "        \"Anger\",\n",
    "        \"Fear\",\n",
    "        \"Love\",\n",
    "        \"Surprise\",\n",
    "    ]\n",
    "    vagueness_labels = [\"No Vagueness\", \"Vagueness\"]\n",
    "    readability_labels = [\n",
    "        \"Extremely Hard\",\n",
    "        \"Hard\",\n",
    "        \"Normal\",\n",
    "        \"Easy\",\n",
    "        \"Very Easy\",\n",
    "        \"Extremely Easy\",\n",
    "    ]\n",
    "\n",
    "    # get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().cpu().numpy())\n",
    "    tokens = [token for token in tokens if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n",
    "    tokens = tokens[: len(pred_labels_stance)]\n",
    "    pred_labels_stance = pred_labels_stance[: len(tokens)]\n",
    "    pred_labels_group_appeals = pred_labels_group_appeals[: len(tokens)]\n",
    "    pred_labels_endorsement = pred_labels_endorsement[: len(tokens)]\n",
    "    pred_labels_hyperbole = pred_labels_hyperbole[: len(tokens)]\n",
    "    pred_labels_sentiment = pred_labels_sentiment[: len(tokens)]\n",
    "    pred_labels_vagueness = pred_labels_vagueness[: len(tokens)]\n",
    "    pred_labels_readability = pred_labels_readability[: len(tokens)]\n",
    "\n",
    "    # get predictions\n",
    "    pred_stance = [stance_labels[label] for label in pred_labels_stance]\n",
    "    pred_group_appeals = [\n",
    "        group_appeals_labels[label] for label in pred_labels_group_appeals\n",
    "    ]\n",
    "    pred_endorsement = [\n",
    "        endorsement_labels[label] for label in pred_labels_endorsement\n",
    "    ]\n",
    "    pred_hyperbole = [hyperbole_labels[label] for label in pred_labels_hyperbole]\n",
    "    pred_sentiment = [\n",
    "        sentiment_labels[label] for label in pred_labels_sentiment\n",
    "    ]\n",
    "    pred_vagueness = [vagueness_labels[label] for label in pred_labels_vagueness]\n",
    "    pred_readability = [\n",
    "        readability_labels[label] for label in pred_labels_readability\n",
    "    ]\n",
    "\n",
    "    # create json\n",
    "    tokens_dict = {}\n",
    "    for i, token in enumerate(tokens):\n",
    "        tokens_dict[token] = {\n",
    "            \"token\": token,\n",
    "            \"stance\": str(pred_labels_stance[i]),\n",
    "            \"group_appeals\": str(pred_labels_group_appeals[i]),\n",
    "            \"endorsement\": str(pred_labels_endorsement[i]),\n",
    "            \"hyperbole\": str(pred_labels_hyperbole[i]),\n",
    "            \"sentiment\": str(pred_labels_sentiment[i]),\n",
    "            \"vagueness\": str(pred_labels_vagueness[i]),\n",
    "            \"readability\": str(pred_labels_readability[i]),\n",
    "        }\n",
    "\n",
    "    json_data = {\n",
    "        \"text\": text,\n",
    "        \"tokens\": tokens_dict,\n",
    "    }\n",
    "    json_file = \"output/output.json\"\n",
    "    with open(json_file, \"w\") as f:\n",
    "        json.dump(json_data, f, indent=4)\n",
    "    print(f\"JSON file saved to {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd120d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertModel,\n",
    ")\n",
    "import json\n",
    "\n",
    "class MultiLabelBERT_Political(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name,\n",
    "        num_stance_labels,\n",
    "        num_group_appeals_labels,\n",
    "        num_endorsement_labels,\n",
    "    ):\n",
    "        super(MultiLabelBERT_Political, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.stance_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_stance_labels\n",
    "        )\n",
    "        self.group_appeals_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_group_appeals_labels\n",
    "        )\n",
    "        self.endorsement_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_endorsement_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        stance_logits = self.stance_classifier(sequence_output)\n",
    "        group_appeals_logits = self.group_appeals_classifier(sequence_output)\n",
    "        endorsement_logits = self.endorsement_classifier(sequence_output)\n",
    "\n",
    "        return stance_logits, group_appeals_logits, endorsement_logits\n",
    "\n",
    "\n",
    "class MultiLabelBERT_Sentiment(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_name, num_hyperbol_labels, num_sentiment_labels):\n",
    "        super(MultiLabelBERT_Sentiment, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.hyperbole_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_hyperbol_labels\n",
    "        )\n",
    "        self.sentiment_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_sentiment_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        hyperbole_logits = self.hyperbole_classifier(sequence_output)\n",
    "        sentiment_logits = self.sentiment_classifier(sequence_output)\n",
    "\n",
    "        return hyperbole_logits, sentiment_logits\n",
    "\n",
    "\n",
    "class MultiLabelBERT_Readability(torch.nn.Module):\n",
    "    \"\"\"BERT model for multi-label token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, bert_model_name, num_vagueness_labels, num_readability_labels):\n",
    "        super(MultiLabelBERT_Readability, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.vagueness_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_vagueness_labels\n",
    "        )\n",
    "        self.readability_classifier = torch.nn.Linear(\n",
    "            self.bert.config.hidden_size, num_readability_labels\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = (\n",
    "            outputs.last_hidden_state\n",
    "        )  # (batch_size, max_len, hidden_size)\n",
    "\n",
    "        vagueness_logits = self.vagueness_classifier(sequence_output)\n",
    "        readability_logits = self.readability_classifier(sequence_output)\n",
    "\n",
    "        return vagueness_logits, readability_logits\n",
    "\n",
    "def main(input_text: str):\n",
    "    # Load the model and tokenizer\n",
    "    model_1 = MultiLabelBERT_Political(\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "        num_stance_labels=3,\n",
    "        num_group_appeals_labels=2,\n",
    "        num_endorsement_labels=2,\n",
    "    )\n",
    "    model_1.load_state_dict(torch.load(model_1_path))\n",
    "    model_1.eval()\n",
    "    model_2 = MultiLabelBERT_Sentiment(\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "        num_hyperbol_labels=2,\n",
    "        num_sentiment_labels=6,\n",
    "    )\n",
    "    model_2.load_state_dict(torch.load(model_2_path))\n",
    "    model_2.eval()\n",
    "    model_3 = MultiLabelBERT_Readability(\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "        num_vagueness_labels=2,\n",
    "        num_readability_labels=6,\n",
    "    )\n",
    "    model_3.load_state_dict(torch.load(model_3_path))\n",
    "    model_3.eval()\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_1.to(device)\n",
    "    model_2.to(device)\n",
    "    model_3.to(device)\n",
    "\n",
    "    # Load the data\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        stance_logits, group_appeals_logits, endorsement_logits = model_1(\n",
    "            inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        )\n",
    "        hyperbole_logits, sentiment_logits = model_2(\n",
    "            inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        )\n",
    "        vagueness_logits, readability_logits = model_3(\n",
    "            inputs[\"input_ids\"], inputs[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "        pred_labels_stance = torch.argmax(stance_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_group_appeals = torch.argmax(group_appeals_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_endorsement = torch.argmax(endorsement_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_hyperbole = torch.argmax(hyperbole_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_sentiment = torch.argmax(sentiment_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_vagueness = torch.argmax(vagueness_logits, dim=2).squeeze().cpu().numpy()\n",
    "        pred_labels_readability = torch.argmax(readability_logits, dim=2).squeeze().cpu().numpy()\n",
    "\n",
    "\n",
    "        # get tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().cpu().numpy())\n",
    "        tokens = [token for token in tokens if token not in [\"[CLS]\", \"[SEP]\", \"[PAD]\"]]\n",
    "        tokens = tokens[: len(pred_labels_stance)]\n",
    "        pred_labels_stance = pred_labels_stance[: len(tokens)]\n",
    "        pred_labels_group_appeals = pred_labels_group_appeals[: len(tokens)]\n",
    "        pred_labels_endorsement = pred_labels_endorsement[: len(tokens)]\n",
    "        pred_labels_hyperbole = pred_labels_hyperbole[: len(tokens)]\n",
    "        pred_labels_sentiment = pred_labels_sentiment[: len(tokens)]\n",
    "        pred_labels_vagueness = pred_labels_vagueness[: len(tokens)]\n",
    "        pred_labels_readability = pred_labels_readability[: len(tokens)]\n",
    "\n",
    "        # # get labels\n",
    "        # stance_labels = [\"Left\", \"Center\", \"Right\"]\n",
    "        # group_appeals_labels = [\"No Group Appeal\", \"Group Appeal\"]\n",
    "        # endorsement_labels = [\"No Endorsement\", \"Endorsement\"]\n",
    "        # hyperbole_labels = [\"No Hyperbole\", \"Hyperbole\"]\n",
    "        # sentiment_labels = [\n",
    "        #     \"Joy\",\n",
    "        #     \"Sadness\",\n",
    "        #     \"Anger\",\n",
    "        #     \"Fear\",\n",
    "        #     \"Love\",\n",
    "        #     \"Surprise\",\n",
    "        # ]\n",
    "        # vagueness_labels = [\"No Vagueness\", \"Vagueness\"]\n",
    "        # readability_labels = [\n",
    "        #     \"Extremely Hard\",\n",
    "        #     \"Hard\",\n",
    "        #     \"Normal\",\n",
    "        #     \"Easy\",\n",
    "        #     \"Very Easy\",\n",
    "        #     \"Extremely Easy\",\n",
    "        # ]\n",
    "\n",
    "        \n",
    "        # # get predictions\n",
    "        # pred_stance = [stance_labels[label] for label in pred_labels_stance]\n",
    "        # pred_group_appeals = [\n",
    "        #     group_appeals_labels[label] for label in pred_labels_group_appeals\n",
    "        # ]\n",
    "        # pred_endorsement = [\n",
    "        #     endorsement_labels[label] for label in pred_labels_endorsement\n",
    "        # ]\n",
    "        # pred_hyperbole = [hyperbole_labels[label] for label in pred_labels_hyperbole]\n",
    "        # pred_sentiment = [\n",
    "        #     sentiment_labels[label] for label in pred_labels_sentiment\n",
    "        # ]\n",
    "        # pred_vagueness = [vagueness_labels[label] for label in pred_labels_vagueness]\n",
    "        # pred_readability = [\n",
    "        #     readability_labels[label] for label in pred_labels_readability\n",
    "        # ]\n",
    "\n",
    "        # create json\n",
    "        tokens_dict = {}\n",
    "        for i, token in enumerate(tokens):\n",
    "            tokens_dict[token] = {\n",
    "                \"token\": token,\n",
    "                \"stance\": str(pred_labels_stance[i]),\n",
    "                \"group_appeals\": str(pred_labels_group_appeals[i]),\n",
    "                \"endorsement\": str(pred_labels_endorsement[i]),\n",
    "                \"hyperbole\": str(pred_labels_hyperbole[i]),\n",
    "                \"sentiment\": str(pred_labels_sentiment[i]),\n",
    "                \"vagueness\": str(pred_labels_vagueness[i]),\n",
    "                \"readability\": str(pred_labels_readability[i]),\n",
    "            }\n",
    "\n",
    "        json_data = {\n",
    "            \"text\": text,\n",
    "            \"tokens\": tokens_dict,\n",
    "        }\n",
    "        json_file = \"output/output.json\"\n",
    "        with open(json_file, \"w\") as f:\n",
    "            json.dump(json_data, f, indent=4)\n",
    "        print(f\"JSON file saved to {json_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
